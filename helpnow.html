<script type="module">

        // Import Whisper from Transformers.js

        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';

        

        // Configure transformers environment

        env.allowRemoteModels = true;

        env.allowLocalModels = false;

        

        // State management

        let currentState = 'idle';

        let detectedLanguage = '';

        let confidenceScore = 0;

        let callStartTime = null;

        let timerInterval = null;

        let microphoneActive = false;

        let whisperPipeline = null;

        let mediaRecorder = null;

        let audioChunks = [];

        let detectionTimeout = null;

        let audioContext = null;

        let source = null;

        let processor = null;

        let stream = null;

        

        // Language mapping for our target languages

        const languageMap = {

            'hi': 'Hindi',

            'zh': 'Mandarin',

            'yue': 'Cantonese', 

            'ar': 'Arabic',

            'vi': 'Vietnamese'

        };

        

        // Initialize Whisper pipeline

        async function initializeWhisper() {

            if (!whisperPipeline) {

                try {

                    console.log('Loading Whisper model... This may take a moment on first use.');

                    // Use the smaller base model for faster loading and processing

                    whisperPipeline = await pipeline('automatic-speech-recognition', 'Xenova/whisper-base');

                    console.log('Whisper model loaded successfully!');

                } catch (error) {

                    console.error('Failed to load Whisper model:', error);

                    throw error;

                }

            }

            return whisperPipeline;

        }